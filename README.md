# Local AI Voice Assistant for Student Learning

> An offline-first, privacy-preserving voice assistant designed for students with reading difficulties, hearing impairments, and digital access challenges.

![Python](https://img.shields.io/badge/Python-3.10+-blue)
![License](https://img.shields.io/badge/License-MIT-green)
![Status](https://img.shields.io/badge/Status-Active-brightgreen)
![RAM Required](https://img.shields.io/badge/RAM-2GB%20Min-orange)

## ğŸ¯ Why This Project?

While most AI tutoring solutions require cloud connectivity and expensive hardware, **students in Tier 2/3 Indian cities need offline, privacy-first learning tools**. This assistant works entirely locallyâ€”no API keys, no internet, no data collection.

**Perfect for:**
- ğŸ“– Students struggling with reading fluency
- ğŸ”Š Students with hearing impairments (captions available)
- ğŸš« Teachers/Parents concerned about data privacy
- ğŸ’» Schools with limited internet bandwidth
- ğŸ’° Budget-conscious educational institutions

## âš¡ Quick Start (3 minutes)

### Prerequisites
- Python 3.10+
- 2GB RAM minimum (4GB recommended)
- ~3GB disk space for models
- macOS, Linux, or Windows (WSL2)

### One-Command Setup

```bash
git clone https://github.com/SanthanaBharathiM/Local-AI-Voice-Assistant-for-Student-Learning
cd local-ai-voice-assistant
chmod +x setup.sh
./setup.sh
```

On Windows (PowerShell):
```powershell
git clone https://github.com/SanthanaBharathiM/Local-AI-Voice-Assistant-for-Student-Learning
cd local-ai-voice-assistant
.\setup.ps1
```

### Run the Assistant

```bash
python src/voice_assistant.py
```

**Output:**
```
ğŸ™ï¸ TEACHING ASSISTANT ONLINE
âœ… System Ready. Listening for your voice...
```

Speak naturally. The assistant will respond in seconds.

---

## ğŸ—ï¸ Architecture

```
User Speech
    â†“
Whisper (Speech-to-Text) â†’ 16kHz audio to text
    â†“
Text Aggregator â†’ Waits for VAD (silence = end of sentence)
    â†“
Llama 3.2 LLM â†’ Generates contextual, encouraging response
    â†“
Kokoro TTS â†’ Converts text to natural, supportive voice
    â†“
Speaker Output â†’ Student hears answer
    â†“
Context Manager â†’ Stores conversation for multi-turn continuity
```

### Tech Stack

| Component | Technology | Why This? |
|-----------|-----------|----------|
| **STT** | Whisper (OpenAI) | Accurate, multilingual, works offline |
| **LLM** | Llama 3.2 (3.2B, Q4_K_M) | Small, quantized, 2GB RAM footprint |
| **TTS** | Kokoro ONNX | Natural voice, supportive tone, CPU-only |
| **Orchestration** | Pipecat | Real-time async pipelines, modular design |
| **Voice Activity Detection** | Silero VAD | Detects conversation endpoints accurately |

---

## ğŸ“‹ What's Included

```
local-ai-voice-assistant/
â”œâ”€â”€ README.md                          # You are here
â”œâ”€â”€ INSTALLATION.md                    # Detailed setup guide
â”œâ”€â”€ ARCHITECTURE.md                    # Technical deep-dive
â”œâ”€â”€ PERFORMANCE.md                     # Benchmarks on different hardware
â”œâ”€â”€ FAQ.md                             # Common issues & fixes
â”œâ”€â”€ CONTRIBUTING.md                    # How to contribute
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ setup.sh                           # Auto-setup script (macOS/Linux)
â”œâ”€â”€ setup.ps1                          # Auto-setup script (Windows)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ voice_assistant.py             # Main entry point
â”‚   â”œâ”€â”€ kokoro_tts.py                  # Kokoro TTS service wrapper
â”‚   â”œâ”€â”€ llama_config.py                # Ollama/Llama configuration
â”‚   â”œâ”€â”€ config.py                      # App settings & parameters
â”‚   â””â”€â”€ system_prompts.py              # AI personality prompts
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ kokoro-v1.0.onnx              # [Auto-downloaded] TTS model
â”‚   â””â”€â”€ voices-v1.0.bin               # [Auto-downloaded] Voice weights
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ student_questions.txt          # Sample Q&A dataset
â”‚   â”œâ”€â”€ custom_prompts.md              # How to customize prompts
â”‚   â””â”€â”€ docker/
â”‚       â””â”€â”€ Dockerfile                 # Containerized setup
â”‚
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ latency_report.md              # Response time analysis
â”‚   â”œâ”€â”€ memory_usage.csv               # RAM consumption metrics
â”‚   â””â”€â”€ accuracy_tests.json            # STT/TTS accuracy data
â”‚
â””â”€â”€ .github/
    â”œâ”€â”€ workflows/
    â”‚   â””â”€â”€ test.yml                   # CI/CD pipeline
    â””â”€â”€ ISSUE_TEMPLATE.md              # Bug report template
```

---

## ğŸš€ Core Features

### âœ… Implemented
- **Voice Conversation**: Real-time speech input â†’ AI response â†’ speech output
- **Context Awareness**: Remembers previous messages in conversation
- **Offline Operation**: Zero internet required after model download
- **Low Resource Usage**: ~2GB RAM, ~300MB active memory during use
- **Encouraging Prompts**: System prompt designed for struggling learners
- **Multiple Voices**: Switch between "af_heart", "am_adam", "bf_emma" (Kokoro)
- **Voice Activity Detection**: Auto-detects when student finishes speaking
- **Async Processing**: Non-blocking audio synthesis, responsive UI

### ğŸ”„ Easy Customization
- Change system prompts (teacher mode, tutor mode, etc.)
- Swap LLM models (Llama 2, Mistral, etc.)
- Add new TTS voices
- Adjust response speed vs quality

---

## ğŸ’¡ Key Technical Decisions

### Decision 1: Q4_K_M Quantization
- **Alternative**: Full-precision Llama 7B (28GB RAM)
- **Trade-off**: ~8-12% accuracy loss vs 10x speed gain
- **Why chosen**: Student learning â‰  creative writing. Speed matters more.

### Decision 2: Kokoro over Piper/glow-tts
- **Alternative 1**: Piper (open-source but robotic)
- **Alternative 2**: glow-tts (better quality but needs GPU)
- **Why chosen**: Natural, encouraging voice on CPU = student engagement

### Decision 3: Pipecat Framework
- **Alternative**: Direct asyncio orchestration
- **Why chosen**: Modular, tested, easy to swap components

---

## ğŸ“Š Performance

### Hardware Requirements

| Hardware | Response Time | Memory Usage | Notes |
|----------|--------------|--------------|-------|
| **M1 MacBook** | 4.2s | 1.8GB | Excellent performance |
| **Intel i7-10700K** | 5.8s | 2.1GB | Good for classroom |
| **AMD Ryzen 5 5500** | 6.2s | 2.0GB | Budget-friendly |
| **Raspberry Pi 4 (8GB)** | 28s | 3.8GB | Educational edge device |

**Note:** First response takes 10-15s (model loading). Subsequent responses are cached.

### Latency Breakdown
```
Student speaks (2 seconds)
  â†“
Whisper STT (0.8s) â€” transcribe audio
  â†“
Text aggregation (0.2s) â€” wait for VAD
  â†“
Llama inference (2.5s) â€” generate response
  â†“
Kokoro synthesis (1.2s) â€” create audio
  â†“
Total: ~6.7 seconds
```

See `benchmarks/latency_report.md` for detailed analysis.

---

## ğŸ”§ Installation Details

### Option 1: Automatic Setup (Recommended)

```bash
./setup.sh
```

This script automatically:
1. Creates Python virtual environment
2. Installs dependencies from `requirements.txt`
3. Downloads and installs Ollama (if not present)
4. Pulls Llama 3.2 model
5. Downloads Kokoro TTS models
6. Verifies everything works

### Option 2: Manual Setup

```bash
# 1. Create virtual environment
python3.10 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Install Ollama
# macOS/Linux: curl -fsSL https://ollama.ai/install.sh | sh
# Windows: Download from https://ollama.ai

# 4. Pull Llama 3.2
ollama pull llama3.2

# 5. Download Kokoro models
python -c "from kokoro_onnx import Kokoro; Kokoro('kokoro-v1.0.onnx', 'voices-v1.0.bin')"

# 6. Start Ollama service (in another terminal)
ollama serve

# 7. Run the assistant
python src/voice_assistant.py
```

**Troubleshooting:** See `INSTALLATION.md` and `FAQ.md`

---

## ğŸ“ Usage Examples

### Basic Conversation

```bash
$ python src/voice_assistant.py

ğŸ™ï¸ TEACHING ASSISTANT ONLINE
âœ… System Ready. Listening for your voice...

[Student speaks]
You: "What is photosynthesis?"

[Processing...]
Assistant: "Think of photosynthesis like plants making their own food. Plants use 
sunlight, water, and air to create energy. It's like they're solar-powered! Does 
that make sense?"
```

### Customize System Prompt

Edit `src/system_prompts.py`:

```python
SCIENCE_TUTOR = """
You are an enthusiastic Science tutor helping students understand concepts 
through analogies and simple examples. Use encouraging language. Keep responses 
to 2-3 sentences. If confused, use a real-world example.
"""

MATH_TUTOR = """
You are a patient Math tutor. Break down problems step-by-step. Use concrete 
examples (money, distance, objects) before abstract formulas.
"""
```

Then in `src/voice_assistant.py`:

```python
from system_prompts import SCIENCE_TUTOR

context = OpenAILLMContext([{"role": "system", "content": SCIENCE_TUTOR}])
```

### Change TTS Voice

```python
# In kokoro_tts.py
voice_options = {
    "supportive_female": "af_heart",      # Default
    "calm_male": "am_adam",
    "warm_female": "bf_emma"
}

# Use different voice
samples, _ = self.tts.create(
    text,
    voice="bf_emma",  # Change here
    speed=0.95,
    lang="en-us"
)
```

### Switch LLM Models

```python
# In src/voice_assistant.py
llm = OLLamaLLMService(
    model="mistral",  # Change from llama3.2
    base_url="http://localhost:11434/v1"
)
```

---

## ğŸ³ Docker Support

Run without any local setup:

```bash
docker build -t voice-assistant .
docker run -it --rm -v /dev/snd:/dev/snd voice-assistant
```

See `examples/docker/Dockerfile` for details.

---

## ğŸ“ˆ Why This Solves Real Problems

| Student Challenge | How This Helps |
|-------------------|---|
| **Can't read fluently** | Hears explanations instead of reading walls of text |
| **No internet at school** | Works completely offline, even in remote areas |
| **Privacy concerns** | No cloud upload, no API calls, data stays on device |
| **Expensive tutoring** | Free software, runs on â‚¹20k budget laptops |
| **Shy about asking questions** | Can ask AI without social anxiety in classroom |
| **Learning disabilities** | Adjustable voice speed, patience, no judgment |

---

## ğŸ” Privacy & Security

âœ… **Zero Cloud Dependency**: No API calls to external services
âœ… **Local Processing**: All data processed on your device
âœ… **No Logging**: Conversations not stored by default
âœ… **No Telemetry**: No tracking or analytics
âœ… **Open Source**: Audit-friendly code, no hidden behavior

**Optional:** Enable local SQLite logging for teacher analytics (see `PRIVACY.md`)

---

## ğŸ¤ Contributing

We welcome contributions! This is actively maintained for:
- New language support
- Additional TTS voices
- Performance optimization
- Educational use case examples
- Documentation improvements

See `CONTRIBUTING.md` for guidelines.

---

## ğŸ“š Documentation

- **[INSTALLATION.md](INSTALLATION.md)** â€” Step-by-step setup guide
- **[ARCHITECTURE.md](ARCHITECTURE.md)** â€” Technical deep-dive with diagrams
- **[PERFORMANCE.md](PERFORMANCE.md)** â€” Benchmarks and optimization tips
- **[FAQ.md](FAQ.md)** â€” Common questions and troubleshooting
- **[CUSTOMIZATION.md](CUSTOMIZATION.md)** â€” How to modify for your needs
- **[PRIVACY.md](PRIVACY.md)** â€” Privacy & data security details

---

## ğŸ¬ Demo Video

Watch a 10-minute walkthrough: [NavGurukul Pre-Work Submission]
- Architecture explanation
- Code walkthrough
- Live demo with students
- Performance benchmarks
- Future roadmap

---

## ğŸ“Š Current Limitations & Roadmap

### Current Limitations
- ğŸ”µ Single-language (English) â€” roadmap: Hindi, Tamil, Gujarati
- ğŸ”µ One voice model â€” roadmap: emotion-based voices
- ğŸ”µ No persistent storage â€” roadmap: optional SQLite logging
- ğŸ”µ No GUI â€” roadmap: web interface for classroom use

### Roadmap (Next 3 months)
- [ ] Response streaming (incremental TTS) â€” cut latency to 3-4s
- [ ] Multi-language support
- [ ] Web UI for classroom management
- [ ] Teacher dashboard (anonymous analytics)
- [ ] Mobile deployment (Termux/Android)
- [ ] Docker compose for school deployments
- [ ] Integration with LMS systems

---

## ğŸŒŸ Who Built This?

**Santhana** â€” ML Engineer, 2+ years in anomaly detection & ML systems

This project was built as part of NavGurukul pre-work, submitted Jan 6, 2026.

---

## ğŸ“ Support

**Having issues?**
1. Check [FAQ.md](FAQ.md) first
2. See [INSTALLATION.md](INSTALLATION.md) troubleshooting section
3. Open an Issue with:
   - Your OS and hardware
   - Full error message
   - Steps to reproduce

**Want to collaborate?**
- Email: [your-email]
- GitHub Issues: [@yourusername/local-ai-voice-assistant]

---

## ğŸ“„ License

MIT License â€” Use freely in educational and commercial projects.

See [LICENSE](LICENSE) for details.

---

## ğŸ™ Acknowledgments

- **Whisper** â€” OpenAI speech recognition
- **Ollama** â€” Local LLM infrastructure
- **Kokoro** â€” Natural TTS voices
- **Pipecat** â€” Real-time AI pipeline framework
- **NavGurukul** â€” Mission to democratize AI education

---

## ğŸ’¬ Citation

If you use this in research or education, please cite:

```bibtex
@software{santhana2026voice_assistant,
  title={Local AI Voice Assistant for Student Learning},
  author={Santhana},
  year={2026},
  url={https://github.com/yourusername/local-ai-voice-assistant}
}
```

---

## ğŸš€ Next Steps

1. **[Quick Start](#-quick-start-3-minutes)** â€” Get running in 3 minutes
2. **[Read Architecture](ARCHITECTURE.md)** â€” Understand the design
3. **[Customize for Your School](CUSTOMIZATION.md)** â€” Adapt prompts/voices
4. **[Deploy](examples/docker/Dockerfile)** â€” Classroom rollout

---

**Made with â¤ï¸ for students who learn differently.**

Last updated: January 6, 2026 | Status: Active & Maintained
